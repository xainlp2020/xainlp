{
    "local-post-hoc": [
     {
      "id": "10",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures",
      "link": "https://www.aclweb.org/anthology/W18-5403.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Semantics: Sentence Level",
      "explainability": "Example-driven, Provenance, Feature Importance",
      "visualization": "Natural Language, Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Layerwise Relevance Propagation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": ""
     },
     {
      "id": "58",
      "authors": "Eric Wallace, Shi Feng, Jordan Boyd-Graber",
      "title": "Interpreting Neural Networks with Nearest Neighbors.",
      "link": "https://www.aclweb.org/anthology/W18-5416.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "10",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "first derivative saliency",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "46",
      "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining Their Explanations",
      "link": "https://www.ijcai.org/Proceedings/2017/0371.pdf",
      "year": "2017",
      "venue": "IJCAI",
      "type": "full",
      "citation": "117",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Input gradient explanations",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": ""
     },
     {
      "id": "15",
      "authors": "Pankaj Gupta, Hinrich Schutze",
      "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation.",
      "link": "https://www.aclweb.org/anthology/W18-5418.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Layerwise Semantic Accumulation",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "49",
      "authors": "Robert Schwarzenberg, David Harbecke, Vivien Macketanz, Eleftherios Avramidis, Sebastian MÃ¶ller",
      "title": "Train, Sort, Explain: Learning to Diagnose Translation Models.",
      "link": "https://www.aclweb.org/anthology/N19-4006.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "3",
      "nlp_task_1": "Interpretability and Analysis of Models for NLP",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "neuron activation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "16",
      "authors": "David Harbecke, Robert Schwarzenberg, Christoph Alt",
      "title": "Learning Explanations from Language Data.",
      "link": "https://www.aclweb.org/anthology/W18-5434.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "1",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "computes saliency but with new metrics borrowed from vision",
      "evaluation_metrics": "none",
      "parts_covered": "applies feature saliency using untested metrics from vision (largely untested for NLP)"
     },
     {
      "id": "3",
      "authors": "David Alvarez-Melis and Tommi S. Jaakkola",
      "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models.",
      "link": "https://www.aclweb.org/anthology/D17-1042.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "66",
      "nlp_task_1": "Machine Translation",
      "explainability": "Surrogate Model",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "input perturbation",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "41",
      "authors": "Nina Poerner, Hinrich Schutze, Benjamin Roth",
      "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.",
      "link": "https://www.aclweb.org/anthology/P18-1032/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "pointing game paradigm + creation of hybrid documents",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high",
      "abstract": "The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",
      "num_preview_img": 2
     },
     {
      "id": "45",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "title": "\"Why Should I Trust You\": Explaining the Predictions of Any Classifier.",
      "link": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
      "year": "2016",
      "venue": "KDD",
      "type": "full",
      "citation": "2701",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Sampling, Word-level Feature Importance and Saliency,",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "Very High"
     },
     {
      "id": "57",
      "authors": "Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp",
      "title": "Learning to Explain Entity Relationships in Knowledge Graphs.",
      "link": "https://www.aclweb.org/anthology/P15-1055/",
      "year": "2015",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Information Retrieval and Text Mining",
      "explainability": "Tree Induction, Surrogate Model, Feature Importance",
      "visualization": "Natural Language",
      "main_explainability": "surrogate model",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Information Retrieval-base, it cast the problem as a ranking problem. Given two entities, they search for sentences that mention the two entities, then rank the sentences using some features.",
      "evaluation_metrics": "none",
      "parts_covered": "high"
     },
     {
      "id": "36",
      "authors": "Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,\nDmitry Ustalov, Simone Paolo Ponzetto, and Chris Biemann",
      "title": "Unsupervised, Knowledge- Free, and Interpretable Word Sense Disambiguation.",
      "link": "https://www.aclweb.org/anthology/D17-2016.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "9",
      "nlp_task_1": "Semantics: Lexical",
      "explainability": "Example-driven, Feature Importance",
      "visualization": "Raw Examples, Saliency, Other Visualization Techniques",
      "main_explainability": "Example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Clustering based on several word features (embeddings, common hypernyms, distributional similarity scores, â¦)\n\nWord-based Saliency",
      "evaluation_metrics": "none",
      "parts_covered": "argues that a user-interface that provides visualization and supporting word sense disambiguation with additional word features (e.g. hypernyms, related sense, ..)"
     }
    ],
    "local-self": [
     {
      "id": "27",
      "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom",
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "link": "https://www.aclweb.org/anthology/P17-1015.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Question Answering",
      "explainability": "Rule Induction",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "generates natural language explanations"
     },
     {
      "id": "34",
      "authors": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein",
      "title": "Explainable Prediction of Medical Codes from Clinical Text.",
      "link": "https://www.aclweb.org/anthology/N18-1100.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "61",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium",
      "abstract": "Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment"
     },
     {
      "id": "59",
      "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy",
      "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.",
      "link": "https://www.aclweb.org/anthology/P17-1088.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "33",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "91",
      "authors": "FrÃ©deric Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, Thomas Demeester",
      "title": "Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?",
      "link": "https://www.aclweb.org/anthology/D18-1365.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high"
     },
     {
      "id": "5",
      "authors": "Malika Aubakirova, Mohit Bansal",
      "title": "Interpreting Neural Networks to Improve Politeness Comprehension.",
      "link": "https://pdfs.semanticscholar.org/331b/2dcd5f6250bd28c6f46cab09b474dce6e9a6.pdf",
      "year": "2016",
      "venue": "EMNLP",
      "type": "short",
      "citation": "22",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency (plus maybe, though leaning towards not adding activation cluster as an operation)",
      "evaluation_metrics": "none",
      "parts_covered": "small (heatmap on important words)"
     },
     {
      "id": "13",
      "authors": "Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli",
      "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/D18-1537.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "19",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) attention saliency\n(b) LSTM gating signals",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": ""
     },
     {
      "id": "21",
      "authors": "Sweta Karlekar, Tong Niu, Mohit Bansal",
      "title": "Detecting Linguistic Characteristics of Alzheimer's Dementia by Interpreting Neural Models.",
      "link": "https://www.aclweb.org/anthology/N18-2110.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "24",
      "authors": "Qiuchi Li, Benyou Wang, Massimo Melucci",
      "title": "CNM: An Interpretable Complex-valued Network for Matching.",
      "link": "https://www.aclweb.org/anthology/N19-1420.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "none",
      "parts_covered": "medium (could be good if someone else reviews it)"
     },
     {
      "id": "54",
      "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal",
      "title": "Generating Token-Level Explanations for Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/N19-1101.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) multiple instance learning (uses thresholded attention to make token-level predictions)\n(b) input perturbation (LIME and Anchor Explanations)",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "9",
      "authors": "Samuel Carton, Qiaozhu Mei, Paul Resnick",
      "title": "Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts",
      "link": "https://www.aclweb.org/anthology/D18-1386.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "adversarial layer on top of attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "32",
      "authors": "Ling Luo, Xiang Ao, Feiyang Pan, Jin Wang, Tong Zhao, Ningzi Yu, Qing He",
      "title": "Beyond Polarity: Interpretable Financial Sentiment Analysis with Hierarchical Query- driven Attention.",
      "link": "https://www.ijcai.org/Proceedings/2018/0590.pdf",
      "year": "2018",
      "venue": "IJCAI",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "query-driven attention mechanism, \n\nhierarchical query-driven attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "8",
      "authors": "Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven Schockaert, Horacio Saggion",
      "title": "Interpretable Emoji Prediction via Label-Wise Attention LSTMs.",
      "link": "https://www.aclweb.org/anthology/D18-1508.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "97",
      "authors": "Yichen Jiang, Mohit Bansal",
      "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.",
      "link": "https://www.aclweb.org/anthology/D19-1455.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Attention, NN-based Controller, Neural Modular Networks",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high"
     },
     {
      "id": "17",
      "authors": "Shiou Tian Hsu, Changsung Moon, Paul Jones, Nagiza F. Samatova",
      "title": "An Interpretable Generative Adversarial Approach to Classification of Latent Entity Relations in Unstructured Sentences.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16629/16066",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Encoder for vector representations and used for reward generation as feedback/labels for a semi-supervised approach to rationale generation and selection.Â",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "93",
      "authors": "Yang Yang, Deyu Zhou, Yulan He, Meng Zhang",
      "title": "Interpretable Relevant Emotion Ranking with Event-Driven Attention.",
      "link": "https://www.aclweb.org/anthology/D19-1017.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Corpus-level and Document-level  Event-Driven attention, \n\nVector products and Matrix Multiplication, SoftmaxÂ  -- various attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "6",
      "authors": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "link": "https://www.aclweb.org/anthology/P18-1208/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "48",
      "nlp_task_1": "Language Grounding to Vision, Robotics and Beyond",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Feature importance where the features are the variables modeling the pair-wise interactions between modalities",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": ""
     },
     {
      "id": "37",
      "authors": "Nikolaos Pappas, Andrei Popescu-Belis",
      "title": "Explaining the Stars: Weighted Multiple-Instance Learning for Aspect- Based Sentiment Analysis.",
      "link": "https://www.aclweb.org/anthology/D14-1052.pdf",
      "year": "2014",
      "venue": "EMNLP",
      "type": "full",
      "citation": "32",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "none",
      "parts_covered": "small"
     },
     {
      "id": "20",
      "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, and Eduard Hovy",
      "title": "Detecting and Explaining Causes From Text For a Time Series Event.",
      "link": "http://www.cs.cmu.edu/~dongyeok/papers/emnlp17_explain.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium"
     },
     {
      "id": "62",
      "authors": "Mantong Zhou, Minlie Huang, Xiaoyan Zhu",
      "title": "Interpretable Reasoning Network for Multi-Relation Question Answering.",
      "link": "https://www.aclweb.org/anthology/C18-1171.pdf",
      "year": "2018",
      "venue": "COLING",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "provenance derivation",
      "evaluation_metrics": "none",
      "parts_covered": "small (last mile, exactly the same as MathQA)"
     },
     {
      "id": "56",
      "authors": "Martin Tutek and Jan Snajder",
      "title": "Iterative Recursive Attention Model for Interpretable Sequence Classification",
      "link": "https://www.aclweb.org/anthology/W18-5427/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "3",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance - Extracted Features",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "12",
      "authors": "Nicolas Garneau, Jean-Samuel Leboeuf, Luc Lamontagne",
      "title": "Predicting and interpreting embeddings for out of vocabulary words in downstream tasks.",
      "link": "https://www.aclweb.org/anthology/W18-5439.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention (on words)",
      "evaluation_metrics": "none",
      "parts_covered": ""
     },
     {
      "id": "22",
      "authors": "Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui, Masaaki Nagata",
      "title": "Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models.",
      "link": "https://www.aclweb.org/anthology/W18-5410.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Summarization",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "66",
      "authors": "Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou, Zenglin Xu",
      "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection.",
      "link": "https://www.aclweb.org/anthology/P19-1006.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "65",
      "authors": "Hui Liu, Qingyu Yin, William Yang Wang",
      "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification.",
      "link": "https://www.aclweb.org/anthology/P19-1560.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "9",
      "nlp_task_1": "",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "comparison to ground truth, human evaluation",
      "parts_covered": "generates prediction from generated explanation"
     },
     {
      "id": "69",
      "authors": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung",
      "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing.",
      "link": "https://www.aclweb.org/anthology/P19-1331/",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "5",
      "nlp_task_1": "Summarization",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "generates an explanation and produces prediction from it"
     },
     {
      "id": "4",
      "authors": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi",
      "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms.",
      "link": "https://www.aclweb.org/anthology/N19-1245.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (\"last mile\")"
     },
     {
      "id": "40",
      "authors": "Pouya Pezeshkpour, Yifan Tian, Sameer Singh",
      "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.",
      "link": "https://www.aclweb.org/anthology/N19-1337/",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "2",
      "nlp_task_1": "Information Extraction",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "adversarial modification (identifies the fact that when removed from the KG changes the prediction for a target fact)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "61",
      "authors": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi",
      "title": "Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.",
      "link": "https://www.aclweb.org/anthology/P18-1101.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "43",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "",
      "evaluation_metrics": "",
      "parts_covered": ""
     },
     {
      "id": "26",
      "authors": "Chao-Chun Liang, Shih-Hong Tsai, Ting-Yun Chang, Yi-Chung Lin, Keh-Yih Su",
      "title": "A Meaning-based English Math Word Problem Solver with Understanding, Reasoning and Explanation.",
      "link": "https://www.aclweb.org/anthology/C16-2032.pdf",
      "year": "2016",
      "venue": "COLING",
      "type": "demo",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Program Induction, Template-based",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "annotates with POS tags",
      "evaluation_metrics": "none",
      "parts_covered": "builds AST"
     },
     {
      "id": "67",
      "authors": "Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal",
      "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension.",
      "link": "https://www.aclweb.org/anthology/P19-1261.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "4",
      "nlp_task_1": "Question Answering",
      "explainability": "Tree Induction, Provenance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "none",
      "parts_covered": "small (last mile?)"
     },
     {
      "id": "2",
      "authors": "Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum",
      "title": "QUINT: Interpretable Question Answering over Knowledge Bases.",
      "link": "https://www.aclweb.org/anthology/D17-2011.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "15",
      "nlp_task_1": "Question Answering",
      "explainability": "Template-based, Provenance, Example-driven",
      "visualization": "Natural Language, other visualization techniques, Raw Examples",
      "main_explainability": "provenance",
      "main_visualization": "natural Language",
      "placement": "2",
      "operations": "template-based, generates query",
      "evaluation_metrics": "none",
      "parts_covered": "template-based, generates query",
      "abstract": "We present QUINT, a live system for question answering over knowledge bases. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question.",
      "num_preview_img": 2
     },
     {
      "id": "35",
      "authors": "An T. Nguyen, Aditya Kharosekar, Matthew Lease, Byron C. Wallace",
      "title": "Interpretable Joint Graphical Model for Fact-Checking From Crowds.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16673/15848",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "17",
      "nlp_task_1": "",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "Provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "entire process"
     },
     {
      "id": "63",
      "authors": "Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba",
      "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
      "link": "https://www.aclweb.org/anthology/P19-1081.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "10",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": "small"
     },
     {
      "id": "64",
      "authors": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher",
      "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
      "link": "https://www.aclweb.org/anthology/P19-1487.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "12",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "Use pretrained language model  that is finetuned on human-provided explanations",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "84",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Auditing Deep Learning processes through Kernel-based Explanatory Models.",
      "link": "https://www.aclweb.org/anthology/D19-1415.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Example-driven",
      "visualization": "Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "2",
      "operations": "layer-wise relevance propagation, surrogate model",
      "evaluation_metrics": "human evaluation",
      "parts_covered": "saliency via layer-wise relevance propagation"
     },
     {
      "id": "70",
      "authors": "Alona Sydorova, Nina Porner, Benjamin Roth",
      "title": "Interpretable Question Answering on Knowledge Bases and Text.",
      "link": "https://www.aclweb.org/anthology/P19-1488.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "2",
      "operations": "attention, input perturbation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium"
     }
    ],
    "global-post-hoc": [
     {
      "id": "44",
      "authors": "Reid Pryzant, Sugato Basu, Kazoo Sone",
      "title": "Interpretable Neural Architectures for Attributing an Ad's Performance to its Writing Style.",
      "link": "https://www.aclweb.org/anthology/W18-5415/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "4",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "class activation mapping + select top k n-grams",
      "evaluation_metrics": "Informal examination",
      "parts_covered": "medium"
     },
     {
      "id": "43",
      "authors": "Reid Pryzant, Kelly Shen, Dan Jurafsky, Stefan Wager",
      "title": "Deconfounded Lexicon Induction for Interpretable Social Science.",
      "link": "https://nlp.stanford.edu/pubs/pryzant2018lexicon.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "6",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "30",
      "authors": "Ninghao Liu, Xiao Huang, Jundong Li, Xia Hu",
      "title": "On Interpretation of Network Embedding via Taxonomy Induction",
      "link": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220001",
      "year": "2018",
      "venue": "KDD",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Surrogate Model",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "surrogate model",
      "main_visualization": "other",
      "placement": "3",
      "operations": "hierarchical clustering on embeddings to infer taxonomy",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     }
    ],
    "global-self": [
     {
      "id": "42",
      "authors": "Nicolas Prollochs, Stefan Feuerriegel, Dirk Neumann",
      "title": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach.",
      "link": "https://www.aclweb.org/anthology/N19-1038.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "4",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "4",
      "operations": "none",
      "evaluation_metrics": "Human evaluation",
      "parts_covered": "learns DFA rules"
     }
    ]
   }