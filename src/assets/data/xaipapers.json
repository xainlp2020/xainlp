{
    "local-post-hoc": [
     {
      "id": "10",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures",
      "link": "https://www.aclweb.org/anthology/W18-5403.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Semantics: Sentence Level",
      "explainability": "Example-driven, Provenance, Feature Importance",
      "visualization": "Natural Language, Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Layerwise Relevance Propagation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "",
      "abstract": "Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called Layerwise Relevance Propagation, over linguistically motivated neural architectures, namely Kernel-based Deep Architectures (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling."
     },
     {
      "id": "58",
      "authors": "Eric Wallace, Shi Feng, Jordan Boyd-Graber",
      "title": "Interpreting Neural Networks with Nearest Neighbors.",
      "link": "https://www.aclweb.org/anthology/W18-5416.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "10",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "first derivative saliency",
      "evaluation_metrics": "none",
      "parts_covered": "medium",
      "abstract": "Local model interpretation methods explain individual predictions by assigning an importance value to each input feature. This value is often determined by measuring the change in confidence when a feature is removed. However, the confidence of neural networks is not a robust measure of model uncertainty. This issue makes reliably judging the importance of the input features difficult. We address this by changing the test-time behavior of neural networks using Deep k-Nearest Neighbors. Without harming text classification accuracy, this algorithm provides a more robust uncertainty metric which we use to generate feature importance values. The resulting interpretations better align with human perception than baseline methods. Finally, we use our interpretation method to analyze model predictions on dataset annotation artifacts."
     },
     {
      "id": "46",
      "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining Their Explanations",
      "link": "https://www.ijcai.org/Proceedings/2017/0371.pdf",
      "year": "2017",
      "venue": "IJCAI",
      "type": "full",
      "citation": "117",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Input gradient explanations",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "abstract": "Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test."
     },
     {
      "id": "15",
      "authors": "Pankaj Gupta, Hinrich Schutze",
      "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation.",
      "link": "https://www.aclweb.org/anthology/W18-5418.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Layerwise Semantic Accumulation",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high",
      "abstract": "Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: “How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response” (2) Example2pattern: “How the saliency patterns look like for each category in the data according to the network in decision making”. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern."
     },
     {
      "id": "49",
      "authors": "Robert Schwarzenberg, David Harbecke, Vivien Macketanz, Eleftherios Avramidis, Sebastian MÃ¶ller",
      "title": "Train, Sort, Explain: Learning to Diagnose Translation Models.",
      "link": "https://www.aclweb.org/anthology/N19-4006.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "3",
      "nlp_task_1": "Interpretability and Analysis of Models for NLP",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "neuron activation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "abstract":"Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity."
     },
     {
      "id": "16",
      "authors": "David Harbecke, Robert Schwarzenberg, Christoph Alt",
      "title": "Learning Explanations from Language Data.",
      "link": "https://www.aclweb.org/anthology/W18-5434.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "1",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "computes saliency but with new metrics borrowed from vision",
      "evaluation_metrics": "none",
      "parts_covered": "applies feature saliency using untested metrics from vision (largely untested for NLP)",
      "abstract": "PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain."
     },
     {
      "id": "3",
      "authors": "David Alvarez-Melis and Tommi S. Jaakkola",
      "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models.",
      "link": "https://www.aclweb.org/anthology/D17-1042.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "66",
      "nlp_task_1": "Machine Translation",
      "explainability": "Surrogate Model",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "input perturbation",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high",
      "abstract": "We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks."
     },
     {
      "id": "41",
      "authors": "Nina Poerner, Hinrich Schutze, Benjamin Roth",
      "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.",
      "link": "https://www.aclweb.org/anthology/P18-1032/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "pointing game paradigm + creation of hybrid documents",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high",
      "abstract": "The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",
      "num_preview_img": 2
     },
     {
      "id": "45",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "title": "\"Why Should I Trust You\": Explaining the Predictions of Any Classifier.",
      "link": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
      "year": "2016",
      "venue": "KDD",
      "type": "full",
      "citation": "2701",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Sampling, Word-level Feature Importance and Saliency,",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "Very High",
      "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted."
     },
     {
      "id": "57",
      "authors": "Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp",
      "title": "Learning to Explain Entity Relationships in Knowledge Graphs.",
      "link": "https://www.aclweb.org/anthology/P15-1055/",
      "year": "2015",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Information Retrieval and Text Mining",
      "explainability": "Tree Induction, Surrogate Model, Feature Importance",
      "visualization": "Natural Language",
      "main_explainability": "surrogate model",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Information Retrieval-base, it cast the problem as a ranking problem. Given two entities, they search for sentences that mention the two entities, then rank the sentences using some features.",
      "evaluation_metrics": "none",
      "parts_covered": "high",
      "abstract":"We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models"
     },
     {
      "id": "36",
      "authors": "Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,\nDmitry Ustalov, Simone Paolo Ponzetto, and Chris Biemann",
      "title": "Unsupervised, Knowledge- Free, and Interpretable Word Sense Disambiguation.",
      "link": "https://www.aclweb.org/anthology/D17-2016.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "9",
      "nlp_task_1": "Semantics: Lexical",
      "explainability": "Example-driven, Feature Importance",
      "visualization": "Raw Examples, Saliency, Other Visualization Techniques",
      "main_explainability": "Example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Clustering based on several word features (embeddings, common hypernyms, distributional similarity scores, â¦)\n\nWord-based Saliency",
      "evaluation_metrics": "none",
      "parts_covered": "argues that a user-interface that provides visualization and supporting word sense disambiguation with additional word features (e.g. hypernyms, related sense, ..)",
      "abstract": "Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration."
     }
    ],
    "local-self": [
     {
      "id": "27",
      "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom",
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "link": "https://www.aclweb.org/anthology/P17-1015.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Question Answering",
      "explainability": "Rule Induction",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "generates natural language explanations",
      "abstract": "Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs."
     },
     {
      "id": "34",
      "authors": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein",
      "title": "Explainable Prediction of Medical Codes from Clinical Text.",
      "link": "https://www.aclweb.org/anthology/N18-1100.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "61",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium",
      "abstract": "Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment"
     },
     {
      "id": "59",
      "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy",
      "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.",
      "link": "https://www.aclweb.org/anthology/P17-1088.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "33",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "abstract":"Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets—WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information."
     },
     {
      "id": "91",
      "authors": "FrÃ©deric Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, Thomas Demeester",
      "title": "Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?",
      "link": "https://www.aclweb.org/anthology/D18-1365.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high",
      "abstract": "Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules."
     },
     {
      "id": "5",
      "authors": "Malika Aubakirova, Mohit Bansal",
      "title": "Interpreting Neural Networks to Improve Politeness Comprehension.",
      "link": "https://pdfs.semanticscholar.org/331b/2dcd5f6250bd28c6f46cab09b474dce6e9a6.pdf",
      "year": "2016",
      "venue": "EMNLP",
      "type": "short",
      "citation": "22",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency (plus maybe, though leaning towards not adding activation cluster as an operation)",
      "evaluation_metrics": "none",
      "parts_covered": "small (heatmap on important words)",
      "abstract":"We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks"
     },
     {
      "id": "13",
      "authors": "Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli",
      "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/D18-1537.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "19",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) attention saliency\n(b) LSTM gating signals",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "abstract": "Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions."
     },
     {
      "id": "21",
      "authors": "Sweta Karlekar, Tong Niu, Mohit Bansal",
      "title": "Detecting Linguistic Characteristics of Alzheimer's Dementia by Interpreting Neural Models.",
      "link": "https://www.aclweb.org/anthology/N18-2110.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency",
      "evaluation_metrics": "Informal Examination",
      "abstract": "Alzheimer’s disease (AD) is an irreversible and progressive brain disease that can be stopped or slowed down with medical treatment. Language changes serve as a sign that a patient’s cognitive functions have been impacted, potentially leading to early diagnosis. In this work, we use NLP techniques to classify and analyze the linguistic characteristics of AD patients using the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs, and their combination, to distinguish between language samples from AD and control patients. We achieve a new independent benchmark accuracy for the AD classification task. More importantly, we next interpret what these neural models have learned about the linguistic characteristics of AD patients, via analysis based on activation clustering and first-derivative saliency techniques. We then perform novel automatic pattern discovery inside activation clusters, and consolidate AD patients’ distinctive grammar patterns. Additionally, we show that first derivative saliency can not only rediscover previous language patterns of AD patients, but also shed light on the limitations of neural models. Lastly, we also include analysis of gender-separated AD data.",
      "parts_covered": "medium"
     },
     {
      "id": "24",
      "authors": "Qiuchi Li, Benyou Wang, Massimo Melucci",
      "title": "CNM: An Interpretable Complex-valued Network for Matching.",
      "link": "https://www.aclweb.org/anthology/N19-1420.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "none",
      "parts_covered": "medium (could be good if someone else reviews it)"
     },
     {
      "id": "54",
      "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal",
      "title": "Generating Token-Level Explanations for Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/N19-1101.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) multiple instance learning (uses thresholded attention to make token-level predictions)\n(b) input perturbation (LIME and Anchor Explanations)",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "9",
      "authors": "Samuel Carton, Qiaozhu Mei, Paul Resnick",
      "title": "Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts",
      "link": "https://www.aclweb.org/anthology/D18-1386.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "adversarial layer on top of attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "32",
      "authors": "Ling Luo, Xiang Ao, Feiyang Pan, Jin Wang, Tong Zhao, Ningzi Yu, Qing He",
      "title": "Beyond Polarity: Interpretable Financial Sentiment Analysis with Hierarchical Query- driven Attention.",
      "link": "https://www.ijcai.org/Proceedings/2018/0590.pdf",
      "year": "2018",
      "venue": "IJCAI",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "query-driven attention mechanism, \n\nhierarchical query-driven attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "8",
      "authors": "Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven Schockaert, Horacio Saggion",
      "title": "Interpretable Emoji Prediction via Label-Wise Attention LSTMs.",
      "link": "https://www.aclweb.org/anthology/D18-1508.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "97",
      "authors": "Yichen Jiang, Mohit Bansal",
      "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.",
      "link": "https://www.aclweb.org/anthology/D19-1455.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Attention, NN-based Controller, Neural Modular Networks",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high"
     },
     {
      "id": "17",
      "authors": "Shiou Tian Hsu, Changsung Moon, Paul Jones, Nagiza F. Samatova",
      "title": "An Interpretable Generative Adversarial Approach to Classification of Latent Entity Relations in Unstructured Sentences.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16629/16066",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Encoder for vector representations and used for reward generation as feedback/labels for a semi-supervised approach to rationale generation and selection.Â",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "93",
      "authors": "Yang Yang, Deyu Zhou, Yulan He, Meng Zhang",
      "title": "Interpretable Relevant Emotion Ranking with Event-Driven Attention.",
      "link": "https://www.aclweb.org/anthology/D19-1017.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Corpus-level and Document-level  Event-Driven attention, \n\nVector products and Matrix Multiplication, SoftmaxÂ  -- various attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "6",
      "authors": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "link": "https://www.aclweb.org/anthology/P18-1208/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "48",
      "nlp_task_1": "Language Grounding to Vision, Robotics and Beyond",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Feature importance where the features are the variables modeling the pair-wise interactions between modalities",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": ""
     },
     {
      "id": "37",
      "authors": "Nikolaos Pappas, Andrei Popescu-Belis",
      "title": "Explaining the Stars: Weighted Multiple-Instance Learning for Aspect- Based Sentiment Analysis.",
      "link": "https://www.aclweb.org/anthology/D14-1052.pdf",
      "year": "2014",
      "venue": "EMNLP",
      "type": "full",
      "citation": "32",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "none",
      "parts_covered": "small"
     },
     {
      "id": "20",
      "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, and Eduard Hovy",
      "title": "Detecting and Explaining Causes From Text For a Time Series Event.",
      "link": "http://www.cs.cmu.edu/~dongyeok/papers/emnlp17_explain.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium"
     },
     {
      "id": "62",
      "authors": "Mantong Zhou, Minlie Huang, Xiaoyan Zhu",
      "title": "Interpretable Reasoning Network for Multi-Relation Question Answering.",
      "link": "https://www.aclweb.org/anthology/C18-1171.pdf",
      "year": "2018",
      "venue": "COLING",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "provenance derivation",
      "evaluation_metrics": "none",
      "parts_covered": "small (last mile, exactly the same as MathQA)"
     },
     {
      "id": "56",
      "authors": "Martin Tutek and Jan Snajder",
      "title": "Iterative Recursive Attention Model for Interpretable Sequence Classification",
      "link": "https://www.aclweb.org/anthology/W18-5427/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "3",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance - Extracted Features",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "12",
      "authors": "Nicolas Garneau, Jean-Samuel Leboeuf, Luc Lamontagne",
      "title": "Predicting and interpreting embeddings for out of vocabulary words in downstream tasks.",
      "link": "https://www.aclweb.org/anthology/W18-5439.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention (on words)",
      "evaluation_metrics": "none",
      "parts_covered": ""
     },
     {
      "id": "22",
      "authors": "Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui, Masaaki Nagata",
      "title": "Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models.",
      "link": "https://www.aclweb.org/anthology/W18-5410.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Summarization",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "66",
      "authors": "Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou, Zenglin Xu",
      "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection.",
      "link": "https://www.aclweb.org/anthology/P19-1006.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium"
     },
     {
      "id": "65",
      "authors": "Hui Liu, Qingyu Yin, William Yang Wang",
      "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification.",
      "link": "https://www.aclweb.org/anthology/P19-1560.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "9",
      "nlp_task_1": "",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "comparison to ground truth, human evaluation",
      "parts_covered": "generates prediction from generated explanation"
     },
     {
      "id": "69",
      "authors": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung",
      "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing.",
      "link": "https://www.aclweb.org/anthology/P19-1331/",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "5",
      "nlp_task_1": "Summarization",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "generates an explanation and produces prediction from it"
     },
     {
      "id": "4",
      "authors": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi",
      "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms.",
      "link": "https://www.aclweb.org/anthology/N19-1245.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (\"last mile\")"
     },
     {
      "id": "40",
      "authors": "Pouya Pezeshkpour, Yifan Tian, Sameer Singh",
      "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.",
      "link": "https://www.aclweb.org/anthology/N19-1337/",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "2",
      "nlp_task_1": "Information Extraction",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "adversarial modification (identifies the fact that when removed from the KG changes the prediction for a target fact)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium"
     },
     {
      "id": "61",
      "authors": "Tiancheng Zhao, Kyusong Lee, Maxine Eskenazi",
      "title": "Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.",
      "link": "https://www.aclweb.org/anthology/P18-1101.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "43",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "",
      "evaluation_metrics": "",
      "parts_covered": ""
     },
     {
      "id": "26",
      "authors": "Chao-Chun Liang, Shih-Hong Tsai, Ting-Yun Chang, Yi-Chung Lin, Keh-Yih Su",
      "title": "A Meaning-based English Math Word Problem Solver with Understanding, Reasoning and Explanation.",
      "link": "https://www.aclweb.org/anthology/C16-2032.pdf",
      "year": "2016",
      "venue": "COLING",
      "type": "demo",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Program Induction, Template-based",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "annotates with POS tags",
      "evaluation_metrics": "none",
      "parts_covered": "builds AST"
     },
     {
      "id": "67",
      "authors": "Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal",
      "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension.",
      "link": "https://www.aclweb.org/anthology/P19-1261.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "4",
      "nlp_task_1": "Question Answering",
      "explainability": "Tree Induction, Provenance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "none",
      "parts_covered": "small (last mile?)"
     },
     {
      "id": "2",
      "authors": "Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum",
      "title": "QUINT: Interpretable Question Answering over Knowledge Bases.",
      "link": "https://www.aclweb.org/anthology/D17-2011.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "15",
      "nlp_task_1": "Question Answering",
      "explainability": "Template-based, Provenance, Example-driven",
      "visualization": "Natural Language, other visualization techniques, Raw Examples",
      "main_explainability": "provenance",
      "main_visualization": "natural Language",
      "placement": "2",
      "operations": "template-based, generates query",
      "evaluation_metrics": "none",
      "parts_covered": "template-based, generates query",
      "abstract": "We present QUINT, a live system for question answering over knowledge bases. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question.",
      "num_preview_img": 2
     },
     {
      "id": "35",
      "authors": "An T. Nguyen, Aditya Kharosekar, Matthew Lease, Byron C. Wallace",
      "title": "Interpretable Joint Graphical Model for Fact-Checking From Crowds.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16673/15848",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "17",
      "nlp_task_1": "",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "Provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "entire process"
     },
     {
      "id": "63",
      "authors": "Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba",
      "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
      "link": "https://www.aclweb.org/anthology/P19-1081.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "10",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": "small"
     },
     {
      "id": "64",
      "authors": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher",
      "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
      "link": "https://www.aclweb.org/anthology/P19-1487.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "12",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "Use pretrained language model  that is finetuned on human-provided explanations",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": ""
     },
     {
      "id": "84",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Auditing Deep Learning processes through Kernel-based Explanatory Models.",
      "link": "https://www.aclweb.org/anthology/D19-1415.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Example-driven",
      "visualization": "Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "2",
      "operations": "layer-wise relevance propagation, surrogate model",
      "evaluation_metrics": "human evaluation",
      "parts_covered": "saliency via layer-wise relevance propagation"
     },
     {
      "id": "70",
      "authors": "Alona Sydorova, Nina Porner, Benjamin Roth",
      "title": "Interpretable Question Answering on Knowledge Bases and Text.",
      "link": "https://www.aclweb.org/anthology/P19-1488.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "2",
      "operations": "attention, input perturbation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium"
     }
    ],
    "global-post-hoc": [
     {
      "id": "44",
      "authors": "Reid Pryzant, Sugato Basu, Kazoo Sone",
      "title": "Interpretable Neural Architectures for Attributing an Ad's Performance to its Writing Style.",
      "link": "https://www.aclweb.org/anthology/W18-5415/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "4",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "class activation mapping + select top k n-grams",
      "evaluation_metrics": "Informal examination",
      "parts_covered": "medium"
     },
     {
      "id": "43",
      "authors": "Reid Pryzant, Kelly Shen, Dan Jurafsky, Stefan Wager",
      "title": "Deconfounded Lexicon Induction for Interpretable Social Science.",
      "link": "https://nlp.stanford.edu/pubs/pryzant2018lexicon.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "6",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "attention",
      "evaluation_metrics": "none",
      "parts_covered": "medium-high"
     },
     {
      "id": "30",
      "authors": "Ninghao Liu, Xiao Huang, Jundong Li, Xia Hu",
      "title": "On Interpretation of Network Embedding via Taxonomy Induction",
      "link": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220001",
      "year": "2018",
      "venue": "KDD",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Surrogate Model",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "surrogate model",
      "main_visualization": "other",
      "placement": "3",
      "operations": "hierarchical clustering on embeddings to infer taxonomy",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": ""
     }
    ],
    "global-self": [
     {
      "id": "42",
      "authors": "Nicolas Prollochs, Stefan Feuerriegel, Dirk Neumann",
      "title": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach.",
      "link": "https://www.aclweb.org/anthology/N19-1038.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "4",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "4",
      "operations": "none",
      "evaluation_metrics": "Human evaluation",
      "parts_covered": "learns DFA rules"
     }
    ]
   }