<!-- <div class="w3-content w3-margin-top" style="max-width:1500px;"></div> -->
<div class="w3-content w3-margin-top w3-padding w3-light-grey def-container">
    <mat-card class="w3-margin def-section">
      <mat-card-title>
        Taxonomy
      </mat-card-title>
      <mat-card-content>
        <mat-card class="w3-content w3-center w3-white w3-margin-top w3-margin-bottom def-card">
          <img src="assets/img/taxonomy.svg">
        </mat-card>
      </mat-card-content>
    </mat-card>


    <mat-card class="w3-margin def-section">
      <mat-card-title>
        Explainability Techniques
      </mat-card-title>
      <mat-card-content>
        <div fxLayout="row wrap" class="w3-padding">


          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Feature Importance</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                Prediction explanations are derived from the
                 importance scores of different features used
                 in the NLP model to output the final prediction.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Surrogate Model</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                Explains model predictions by learning a second model,
                which is usually more explainable than the original
                model, as a proxy.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Example-Driven</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                A particular prediction of an input instance is explained
                by identifying and presenting other instances, usually from
                available labeled data, that are semantically similar to the input instance.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Provenance</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                The explanation is an illustration of the entire prediction
                derivation process, which is an intuitive and effective
                explainability technique when the final prediction consists
                of a series of reasoning steps.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Induction</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
              Explanations are generated by inducing human-readable
              representations, such as rules, trees and programs.
              </div>
            </mat-card-content>
          </mat-card>
        </div>

      </mat-card-content>

    </mat-card>


    <mat-card class="w3-margin def-section">
      <mat-card-title>
        Visualization Techniques - NOT FINISHED
      </mat-card-title>
      <mat-card-content>

        <div  class="w3-padding">

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Saliency - Heatmap</mat-card-title>
            </mat-card-header>
            <mat-card-content fxLayout="column" style="display:flex;">
              <div>
                <img width="230px" style="margin-right:50px; margin-left:8px;" src="assets/img/saliency-heatmap-word-alignment.png">
                <div class="cite">
                  <a target="_blank" rel="noopener noreferrer"
                  href="https://arxiv.org/pdf/1409.0473.pdf">
                  (Bahdanau et al.,2015)</a>
                </div>
            </div>
            <div style="display: flex; align-items: center;">
              <span class="text">
                Saliency has been primarily used to visualize the importance
                scores  of  different  types  of  elements  in  XAI  learning
                systems,  such  as  showing input-output word alignment.
              </span>
            </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Saliency - Highlighting</mat-card-title>
            </mat-card-header>
            <mat-card-content fxLayout="column" style="display:flex;">
              <div>
                <img width="280px" style="margin:8px;" src="assets/img/saliency-highlighting.png">
                <div class="cite">
                  <a target="_blank" rel="noopener noreferrer"
                  href="https://www.aclweb.org/anthology/N18-1100/">
                  (Mullenbach et al., 2018)</a>
                </div>
            </div>
            <div style="display: flex; align-items: center;">
              <span class="text">
                Saliency has been primarily used to visualize the importance
                scores of different types of elements in XAI learning
                systems, such as highlighting words in input text.
              </span>
            </div>
            </mat-card-content>
          </mat-card>

        </div>

      </mat-card-content>

    </mat-card>


    <mat-card class="w3-margin def-section">
      <mat-card-title>
        Evaluation Metrics
      </mat-card-title>
      <mat-card-content>

        <div fxLayout="row wrap" class="w3-padding">


          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Informal Examination</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                A presentation of no more than a few generated explanations,
                usually accompanied by a high-level discussion on how
                they align with human intuition.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Comparison to Ground Truth</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                Compare generated explanations to ground truth data in an effort to
                quantify the performance of explainability techniques. Employed metrics
                vary based on task and explainability technique.
              </div>
            </mat-card-content>
          </mat-card>

          <mat-card class="def-card">
            <mat-card-header class="title">
              <mat-card-title>Human Evaluation</mat-card-title>
            </mat-card-header>
             <mat-card-content>
              <div class="text">
                Humans directly evaluate the effectiveness of the generated
                explanations via one or more user studies.
              </div>
            </mat-card-content>
          </mat-card>

          </div>

      </mat-card-content>

    </mat-card>



  </div>

