{
    "local-post-hoc": [
     {
      "id": "10",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures",
      "link": "https://www.aclweb.org/anthology/W18-5403.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Semantics: Sentence Level",
      "explainability": "Example-driven, Provenance, Feature Importance",
      "visualization": "Natural Language, Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Layerwise Relevance Propagation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract": "Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called Layerwise Relevance Propagation, over linguistically motivated neural architectures, namely Kernel-based Deep Architectures (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling."
     },
     {
      "id": "58",
      "authors": "Eric Wallace, Shi Feng, Jordan Boyd-Graber",
      "title": "Interpreting Neural Networks with Nearest Neighbors.",
      "link": "https://www.aclweb.org/anthology/W18-5416.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "10",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "first derivative saliency",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract": "Local model interpretation methods explain individual predictions by assigning an importance value to each input feature. This value is often determined by measuring the change in confidence when a feature is removed. However, the confidence of neural networks is not a robust measure of model uncertainty. This issue makes reliably judging the importance of the input features difficult. We address this by changing the test-time behavior of neural networks using Deep k-Nearest Neighbors. Without harming text classification accuracy, this algorithm provides a more robust uncertainty metric which we use to generate feature importance values. The resulting interpretations better align with human perception than baseline methods. Finally, we use our interpretation method to analyze model predictions on dataset annotation artifacts."
     },
     {
      "id": "46",
      "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining Their Explanations",
      "link": "https://www.ijcai.org/Proceedings/2017/0371.pdf",
      "year": "2017",
      "venue": "IJCAI",
      "type": "full",
      "citation": "117",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Input gradient explanations",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract": "Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test."
     },
     {
      "id": "15",
      "authors": "Pankaj Gupta, Hinrich Schutze",
      "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation.",
      "link": "https://www.aclweb.org/anthology/W18-5418.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "5",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Layerwise Semantic Accumulation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high",
      "num_preview_img": 3,
      "abstract": "Recurrent neural networks (RNNs) are temporal networks and cumulative in nature that have shown promising results in various natural language processing tasks. Despite their success, it still remains a challenge to understand their hidden behavior. In this work, we analyze and interpret the cumulative nature of RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation (LISA) for explaining decisions and detecting the most likely (i.e., saliency) patterns that the network relies on while decision making. We demonstrate (1) LISA: “How an RNN accumulates or builds semantics during its sequential processing for a given text example and expected response” (2) Example2pattern: “How the saliency patterns look like for each category in the data according to the network in decision making”. We analyse the sensitiveness of RNNs about different inputs to check the increase or decrease in prediction scores and further extract the saliency patterns learned by the network. We employ two relation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to explain RNN predictions via the LISA and example2pattern."
     },
     {
      "id": "49",
      "authors": "Robert Schwarzenberg, David Harbecke, Vivien Macketanz, Eleftherios Avramidis, Sebastian MÃ¶ller",
      "title": "Train, Sort, Explain: Learning to Diagnose Translation Models.",
      "link": "https://www.aclweb.org/anthology/N19-4006.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "3",
      "nlp_task_1": "Interpretability and Analysis of Models for NLP",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "neuron activation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75% and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity."
     },
     {
      "id": "16",
      "authors": "David Harbecke, Robert Schwarzenberg, Christoph Alt",
      "title": "Learning Explanations from Language Data.",
      "link": "https://www.aclweb.org/anthology/W18-5434.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "1",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "computes saliency but with new metrics borrowed from vision",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "applies feature saliency using untested metrics from vision (largely untested for NLP)",
      "num_preview_img": 2,
      "abstract": "PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain."
     },
     {
      "id": "3",
      "authors": "David Alvarez-Melis and Tommi S. Jaakkola",
      "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models.",
      "link": "https://www.aclweb.org/anthology/D17-1042.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "66",
      "nlp_task_1": "Machine Translation",
      "explainability": "Surrogate Model",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "input perturbation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high",
      "num_preview_img": 2,
      "abstract": "We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an “explanation” consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks."
     },
     {
      "id": "41",
      "authors": "Nina Poerner, Hinrich Schutze, Benjamin Roth",
      "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.",
      "link": "https://www.aclweb.org/anthology/P18-1032/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "pointing game paradigm + creation of hybrid documents",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high",
      "abstract": "The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",
      "num_preview_img": 2
     },
     {
      "id": "45",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "title": "\"Why Should I Trust You\": Explaining the Predictions of Any Classifier.",
      "link": "https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf",
      "year": "2016",
      "venue": "KDD",
      "type": "full",
      "citation": "2701",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "1",
      "operations": "Sampling, Word-level Feature Importance and Saliency,",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "Very High",
      "num_preview_img": 3,
      "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted."
     },
     {
      "id": "57",
      "authors": "Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke, and Wouter Weerkamp",
      "title": "Learning to Explain Entity Relationships in Knowledge Graphs.",
      "link": "https://www.aclweb.org/anthology/P15-1055/",
      "year": "2015",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Information Retrieval and Text Mining",
      "explainability": "Tree Induction, Surrogate Model, Feature Importance",
      "visualization": "Natural Language",
      "main_explainability": "surrogate model",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Information Retrieval-base, it cast the problem as a ranking problem. Given two entities, they search for sentences that mention the two entities, then rank the sentences using some features.",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "high",
	  "num_preview_img": 2,
      "abstract":"We study the problem of explaining relationships between pairs of knowledge graph entities with human-readable descriptions. Our method extracts and enriches sentences that refer to an entity pair from a corpus and ranks the sentences according to how well they describe the relationship between the entities. We model this task as a learning to rank problem for sentences and employ a rich set of features. When evaluated on a large set of manually annotated sentences, we find that our method significantly improves over state-of-the-art baseline models"
     },
     {
      "id": "36",
      "authors": "Alexander Panchenko, Fide Marten, Eugen Ruppert, Stefano Faralli,\nDmitry Ustalov, Simone Paolo Ponzetto, and Chris Biemann",
      "title": "Unsupervised, Knowledge- Free, and Interpretable Word Sense Disambiguation.",
      "link": "https://www.aclweb.org/anthology/D17-2016.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "9",
      "nlp_task_1": "Semantics: Lexical",
      "explainability": "Example-driven, Feature Importance",
      "visualization": "Raw Examples, Saliency, Other Visualization Techniques",
      "main_explainability": "Example-driven",
      "main_visualization": "raw examples",
      "placement": "1",
      "operations": "Clustering based on several word features (embeddings, common hypernyms, distributional similarity scores, â¦)\n\nWord-based Saliency",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "argues that a user-interface that provides visualization and supporting word sense disambiguation with additional word features (e.g. hypernyms, related sense, ..)",
      "num_preview_img": 2,
      "abstract": "Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration."
     }
    ],
    "local-self": [
     {
      "id": "27",
      "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom",
      "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
      "link": "https://www.aclweb.org/anthology/P17-1015.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "44",
      "nlp_task_1": "Question Answering",
      "explainability": "Rule Induction",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "generates natural language explanations",
      "num_preview_img": 1,
      "abstract": "Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs."
     },
     {
      "id": "34",
      "authors": "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein",
      "title": "Explainable Prediction of Medical Codes from Clinical Text.",
      "link": "https://www.aclweb.org/anthology/N18-1100.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "61",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium",
      "num_preview_img": 2,
      "abstract": "Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment"
     },
     {
      "id": "59",
      "authors": "Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy",
      "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.",
      "link": "https://www.aclweb.org/anthology/P17-1088.pdf",
      "year": "2017",
      "venue": "ACL",
      "type": "full",
      "citation": "33",
      "nlp_task_1": "",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets—WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information."
     },
     {
      "id": "91",
      "authors": "FrÃ©deric Godin, Kris Demuynck, Joni Dambre, Wesley De Neve, Thomas Demeester",
      "title": "Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?",
      "link": "https://www.aclweb.org/anthology/D18-1365.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "medium-high",
      "num_preview_img": 2,
      "abstract": "Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules."
     },
     {
      "id": "5",
      "authors": "Malika Aubakirova, Mohit Bansal",
      "title": "Interpreting Neural Networks to Improve Politeness Comprehension.",
      "link": "https://pdfs.semanticscholar.org/331b/2dcd5f6250bd28c6f46cab09b474dce6e9a6.pdf",
      "year": "2016",
      "venue": "EMNLP",
      "type": "short",
      "citation": "22",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency (plus maybe, though leaning towards not adding activation cluster as an operation)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (heatmap on important words)",
      "num_preview_img": 2,
      "abstract":"We present an interpretable neural network approach to predicting and understanding politeness in natural language requests. Our models are based on simple convolutional neural networks directly on raw text, avoiding any manual identification of complex sentiment or syntactic features, while performing better than such feature-based models from previous work. More importantly, we use the challenging task of politeness prediction as a testbed to next present a much-needed understanding of what these successful networks are actually learning. For this, we present several network visualizations based on activation clusters, first derivative saliency, and embedding space transformations, helping us automatically identify several subtle linguistics markers of politeness theories. Further, this analysis reveals multiple novel, high-scoring politeness strategies which, when added back as new features, reduce the accuracy gap between the original featurized system and the neural model, thus providing a clear quantitative interpretation of the success of these neural networks"
     },
     {
      "id": "13",
      "authors": "Reza Ghaeini, Xiaoli Z. Fern, Prasad Tadepalli",
      "title": "Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/D18-1537.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "19",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) attention saliency\n(b) LSTM gating signals",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract": "Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions."
     },
     {
      "id": "21",
      "authors": "Sweta Karlekar, Tong Niu, Mohit Bansal",
      "title": "Detecting Linguistic Characteristics of Alzheimer's Dementia by Interpreting Neural Models.",
      "link": "https://www.aclweb.org/anthology/N18-2110.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "first derivative saliency",
      "evaluation_metrics": "Informal Examination",
      "num_preview_img": 2,
      "abstract": "Alzheimer’s disease (AD) is an irreversible and progressive brain disease that can be stopped or slowed down with medical treatment. Language changes serve as a sign that a patient’s cognitive functions have been impacted, potentially leading to early diagnosis. In this work, we use NLP techniques to classify and analyze the linguistic characteristics of AD patients using the DementiaBank dataset. We apply three neural models based on CNNs, LSTM-RNNs, and their combination, to distinguish between language samples from AD and control patients. We achieve a new independent benchmark accuracy for the AD classification task. More importantly, we next interpret what these neural models have learned about the linguistic characteristics of AD patients, via analysis based on activation clustering and first-derivative saliency techniques. We then perform novel automatic pattern discovery inside activation clusters, and consolidate AD patients’ distinctive grammar patterns. Additionally, we show that first derivative saliency can not only rediscover previous language patterns of AD patients, but also shed light on the limitations of neural models. Lastly, we also include analysis of gender-separated AD data.",
      "parts_covered": "medium"
     },
     {
      "id": "24",
      "authors": "Qiuchi Li, Benyou Wang, Massimo Melucci",
      "title": "CNM: An Interpretable Complex-valued Network for Matching.",
      "link": "https://www.aclweb.org/anthology/N19-1420.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium (could be good if someone else reviews it)",
      "num_preview_img": 1,
      "abstract":"This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets."
     },
     {
      "id": "54",
      "authors": "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal",
      "title": "Generating Token-Level Explanations for Natural Language Inference",
      "link": "https://www.aclweb.org/anthology/N19-1101.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Feature Importance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "(a) multiple instance learning (uses thresholded attention to make token-level predictions)\n(b) input perturbation (LIME and Anchor Explanations)",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods."
     },
     {
      "id": "9",
      "authors": "Samuel Carton, Qiaozhu Mei, Paul Resnick",
      "title": "Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts",
      "link": "https://www.aclweb.org/anthology/D18-1386.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "adversarial layer on top of attention",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate “default” behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes."
     },
     {
      "id": "32",
      "authors": "Ling Luo, Xiang Ao, Feiyang Pan, Jin Wang, Tong Zhao, Ningzi Yu, Qing He",
      "title": "Beyond Polarity: Interpretable Financial Sentiment Analysis with Hierarchical Query- driven Attention.",
      "link": "https://www.ijcai.org/Proceedings/2018/0590.pdf",
      "year": "2018",
      "venue": "IJCAI",
      "type": "full",
      "citation": "16",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "query-driven attention mechanism, \n\nhierarchical query-driven attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 2,
      "abstract":"Sentiment analysis has played a significant role in financial applications in recent years. The informational and emotive aspects of news texts may affect the prices, volatilities, volume of trades, and even potential risks of financial subjects. Previous studies in this field mainly focused on identifying polarity~(e.g. positive or negative). However, as financial decisions broadly require justifications, only plausible polarity cannot provide enough evidence during the decision making processes of humanity. Hence an explainable solution is in urgent demand. In this paper, we present an interpretable neural net framework for financial sentiment analysis. First, we design a hierarchical model to learn the representation of a document from multiple granularities. In addition, we propose a query-driven attention mechanism to satisfy the unique characteristics of financial documents. With the domain specified questions provided by the financial analysts, we can discover different spotlights for queries from different aspects. We conduct extensive experiments on a real-world dataset. The results demonstrate that our framework can learn better representation of the document and unearth meaningful clues on replying different users? preferences. It also outperforms the state-of-the-art methods on sentiment prediction of financial documents. "
     },
     {
      "id": "8",
      "authors": "Francesco Barbieri, Luis Espinosa-Anke, Jose Camacho-Collados, Steven Schockaert, Horacio Saggion",
      "title": "Interpretable Emoji Prediction via Label-Wise Attention LSTMs.",
      "link": "https://www.aclweb.org/anthology/D18-1508.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "short",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Human language has evolved towards newer forms of communication such as social media, where emojis (i.e., ideograms bearing a visual meaning) play a key role. While there is an increasing body of work aimed at the computational modeling of emoji semantics, there is currently little understanding about what makes a computational model represent or predict a given emoji in a certain way. In this paper we propose a label-wise attention mechanism with which we attempt to better understand the nuances underlying emoji prediction. In addition to advantages in terms of interpretability, we show that our proposed architecture improves over standard baselines in emoji prediction, and does particularly well when predicting infrequent emojis."
     },
     {
      "id": "97",
      "authors": "Yichen Jiang, Mohit Bansal",
      "title": "Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning.",
      "link": "https://www.aclweb.org/anthology/D19-1455.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Attention, NN-based Controller, Neural Modular Networks",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high",
      "num_preview_img": 2,
      "abstract":"Multi-hop QA requires a model to connect multiple pieces of evidence scattered in a long context to answer the question. The recently proposed HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four different multi-hop reasoning paradigms (two bridge entity setups, checking multiple properties, and comparing two entities), making it challenging for a single neural network to handle all four. In this work, we present an interpretable, controller-based Self-Assembling Neural Modular Network (Hu et al., 2017, 2018) for multi-hop reasoning, where we design four novel modules (Find, Relocate, Compare, NoOp) to perform unique types of language reasoning. Based on a question, our layout controller RNN dynamically infers a series of reasoning modules to construct the entire network. Empirically, we show that our dynamic, multi-hop modular network achieves significant improvements over the static, single-hop baseline (on both regular and adversarial evaluation). We further demonstrate the interpretability of our model via three analyses. First, the controller can softly decompose the multi-hop question into multiple single-hop sub-questions to promote compositional reasoning behavior of the main network. Second, the controller can predict layouts that conform to the layouts designed by human experts. Finally, the intermediate module can infer the entity that connects two distantly-located supporting facts by addressing the sub-question from the controller."
     },
     {
      "id": "17",
      "authors": "Shiou Tian Hsu, Changsung Moon, Paul Jones, Nagiza F. Samatova",
      "title": "An Interpretable Generative Adversarial Approach to Classification of Latent Entity Relations in Unstructured Sentences.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16629/16066",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Information Extraction",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Encoder for vector representations and used for reward generation as feedback/labels for a semi-supervised approach to rationale generation and selection.Â",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"We propose a generative adversarial neural network model for relation classification that attempts to emulate the way in which human analysts might process sentences. Our approach provides two unique benefits over existing capabilities: (1) we make predictions by finding and exploiting supportive rationales to improve interpretability (i.e. words or phrases extracted from a sentence that a person can reason upon), and (2) we allow predictions to be easily corrected by adjusting the rationales.Our model consists of three stages: Generator, Selector, and Encoder. The Generator identifies candidate text fragments; the Selector decides which fragments can be used as rationales depending on the goal; and finally, the Encoder performs relation reasoning on the rationales. While the Encoder is trained in a supervised manner to classify relations, the Generator and Selector are designed as unsupervised models to identify rationales without prior knowledge, although they can be semi-supervised through human annotations. We evaluate our model on data from SemEval 2010 that provides 19 relation-classes. Experiments demonstrate that our approach outperforms state-of-the-art models, and that our model is capable of extracting good rationales on its own as well as benefiting from labeled rationales if provided."
     },
     {
      "id": "93",
      "authors": "Yang Yang, Deyu Zhou, Yulan He, Meng Zhang",
      "title": "Interpretable Relevant Emotion Ranking with Event-Driven Attention.",
      "link": "https://www.aclweb.org/anthology/D19-1017.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Corpus-level and Document-level  Event-Driven attention, \n\nVector products and Matrix Multiplication, SoftmaxÂ  -- various attention mechanisms",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Multiple emotions with different intensities are often evoked by events described in documents. Oftentimes, such event information is hidden and needs to be discovered from texts. Unveiling the hidden event information can help to understand how the emotions are evoked and provide explainable results. However, existing studies often ignore the latent event information. In this paper, we proposed a novel interpretable relevant emotion ranking model with the event information incorporated into a deep learning architecture using the event-driven attentions. Moreover, corpus-level event embeddings and document-level event distributions are introduced respectively to consider the global events in corpus and the document-specific events simultaneously. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label approaches. Moreover, interpretable results can be obtained to shed light on the events which trigger certain emotions."
     },
     {
      "id": "6",
      "authors": "AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "link": "https://www.aclweb.org/anthology/P18-1208/",
      "year": "2018",
      "venue": "ACL",
      "type": "full",
      "citation": "48",
      "nlp_task_1": "Language Grounding to Vision, Robotics and Beyond",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "Feature importance where the features are the variables modeling the pair-wise interactions between modalities",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art."
     },
     {
      "id": "37",
      "authors": "Nikolaos Pappas, Andrei Popescu-Belis",
      "title": "Explaining the Stars: Weighted Multiple-Instance Learning for Aspect- Based Sentiment Analysis.",
      "link": "https://www.aclweb.org/anthology/D14-1052.pdf",
      "year": "2014",
      "venue": "EMNLP",
      "type": "full",
      "citation": "32",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "none",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small",
      "num_preview_img": 2,
      "abstract":"This paper introduces a model of multipleinstance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from usercontributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multipleinstance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE."
     },
     {
      "id": "20",
      "authors": "Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen, and Eduard Hovy",
      "title": "Detecting and Explaining Causes From Text For a Time Series Event.",
      "link": "http://www.cs.cmu.edu/~dongyeok/papers/emnlp17_explain.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "NLP Applications",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium",
      "num_preview_img": 2,
      "abstract":"Explaining underlying causes or effects about events is a challenging but valuable task. We define a novel problem of generating explanations of a time series event by (1) searching cause and effect relationships of the time series with textual data and (2) constructing a connecting chain between them to generate an explanation. To detect causal features from text, we propose a novel method based on the Granger causality of time series between features extracted from text such as N-grams, topics, sentiments, and their composition. The generation of the sequence of causal entities requires a commonsense causative knowledge base with efficient reasoning. To ensure good interpretability and appropriate lexical usage we combine symbolic and neural representations, using a neural reasoning algorithm trained on commonsense causal tuples to predict the next cause step. Our quantitative and human analysis show empirical evidence that our method successfully extracts meaningful causality relationships between time series with textual features and generates appropriate explanation between them."
     },
     {
      "id": "62",
      "authors": "Mantong Zhou, Minlie Huang, Xiaoyan Zhu",
      "title": "Interpretable Reasoning Network for Multi-Relation Question Answering.",
      "link": "https://www.aclweb.org/anthology/C18-1171.pdf",
      "year": "2018",
      "venue": "COLING",
      "type": "full",
      "citation": "11",
      "nlp_task_1": "Question Answering",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "provenance derivation",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (last mile, exactly the same as MathQA)",
      "num_preview_img": 1,
      "abstract":"Multi-relation Question Answering is a challenging task, due to the requirement of elaborated analysis on questions and reasoning over multiple fact triples in knowledge base. In this paper, we present a novel model called Interpretable Reasoning Network that employs an interpretable, hop-by-hop reasoning process for question answering. The model dynamically decides which part of an input question should be analyzed at each hop; predicts a relation that corresponds to the current parsed results; utilizes the predicted relation to update the question representation and the state of the reasoning process; and then drives the next-hop reasoning. Experiments show that our model yields state-of-the-art results on two datasets. More interestingly, the model can offer traceable and observable intermediate predictions for reasoning analysis and failure diagnosis, thereby allowing manual manipulation in predicting the final answer."
     },
     {
      "id": "56",
      "authors": "Martin Tutek and Jan Snajder",
      "title": "Iterative Recursive Attention Model for Interpretable Sequence Classification",
      "link": "https://www.aclweb.org/anthology/W18-5427/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "3",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Feature Importance - Extracted Features",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high",
      "num_preview_img": 1,
      "abstract":"Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of inference steps. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our model on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art."
     },
     {
      "id": "12",
      "authors": "Nicolas Garneau, Jean-Samuel Leboeuf, Luc Lamontagne",
      "title": "Predicting and interpreting embeddings for out of vocabulary words in downstream tasks.",
      "link": "https://www.aclweb.org/anthology/W18-5439.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention (on words)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a network that predicts useful embeddings for OOV words based on their morphology and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the left context words, the right context words or the word’s characters, hence making the prediction more interpretable. The model is a “drop-in” module that is jointly trained with the downstream task’s neural network, thus producing embeddings specialized for the task at hand. When the task is mostly syntactical, we observe that our model aims most of its attention on surface form characters. On the other hand, for tasks more semantical, the network allocates more attention to the surrounding words. In all our tests, the module helps the network to achieve better performances in comparison to the use of simple random embeddings."
     },
     {
      "id": "22",
      "authors": "Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki Okazaki, Kentaro Inui, Masaaki Nagata",
      "title": "Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models.",
      "link": "https://www.aclweb.org/anthology/W18-5410.pdf",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "2",
      "nlp_task_1": "Summarization",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor. Conventionally, many studies have used an attention matrix to interpret how Encoder-Decoder-based models translate a given source sentence to the corresponding target sentence. However, recent studies have empirically revealed that an attention matrix is not optimal for token-wise translation analyses. We propose a method that explicitly models the token-wise alignment between the source and target sequences to provide a better analysis. Experiments show that our method can acquire token-wise alignments that are superior to those of an attention mechanism."
     },
     {
      "id": "66",
      "authors": "Junyu Lu, Chenbin Zhang, Zeying Xie, Guang Ling, Tom Chao Zhou, Zenglin Xu",
      "title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection.",
      "link": "https://www.aclweb.org/anthology/P19-1006.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "short",
      "citation": "1",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "Feature Importance",
      "main_visualization": "Saliency",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching."
     },
     {
      "id": "65",
      "authors": "Hui Liu, Qingyu Yin, William Yang Wang",
      "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification.",
      "link": "https://www.aclweb.org/anthology/P19-1560.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "9",
      "nlp_task_1": "",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "comparison to ground truth, human evaluation",
      "parts_covered": "generates prediction from generated explanation",
      "num_preview_img": 1,
      "abstract":"Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time."
     },
     {
      "id": "69",
      "authors": "Yue Dong, Zichao Li, Mehdi Rezagholizadeh, Jackie Chi Kit Cheung",
      "title": "EditNTS: An Neural Programmer-Interpreter Model for Sentence Simplification through Explicit Editing.",
      "link": "https://www.aclweb.org/anthology/P19-1331/",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "5",
      "nlp_task_1": "Summarization",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "generates an explanation and produces prediction from it",
      "num_preview_img": 1,
      "abstract":"We present the first sentence simplification model that learns explicit edit operations (ADD, DELETE, and KEEP) via a neural programmer-interpreter approach. Most current neural sentence simplification systems are variants of sequence-to-sequence models adopted from machine translation. These methods learn to simplify sentences as a byproduct of the fact that they are trained on complex-simple sentence pairs. By contrast, our neural programmer-interpreter is directly trained to predict explicit edit operations on targeted parts of the input sentence, resembling the way that humans perform simplification and revision. Our model outperforms previous state-of-the-art neural sentence simplification models (without external knowledge) by large margins on three benchmark text simplification corpora in terms of SARI (+0.95 WikiLarge, +1.89 WikiSmall, +1.41 Newsela), and is judged by humans to produce overall better and simpler output sentences."
     },
     {
      "id": "4",
      "authors": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi",
      "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms.",
      "link": "https://www.aclweb.org/anthology/N19-1245.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "3",
      "nlp_task_1": "Semantics: Textual Inference and Other Areas of Semantics",
      "explainability": "Program Induction",
      "visualization": "Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (\"last mile\")",
      "num_preview_img": 2,
      "abstract":"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver by learning to map problems to their operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, we significantly enhance the AQUA-RAT dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model with automatic problem categorization. Our experiments show improvements over competitive baselines in our dataset as well as the AQUA-RAT dataset. The results are still lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at https://math-qa.github.io/math-QA/"
     },
     {
      "id": "40",
      "authors": "Pouya Pezeshkpour, Yifan Tian, Sameer Singh",
      "title": "Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications.",
      "link": "https://www.aclweb.org/anthology/N19-1337/",
      "year": "2019",
      "venue": "NAACL",
      "type": "full",
      "citation": "2",
      "nlp_task_1": "Information Extraction",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "adversarial modification (identifies the fact that when removed from the KG changes the prediction for a target fact)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium",
      "num_preview_img": 1,
      "abstract":"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base."
     },
     {
      "id": "26",
      "authors": "Chao-Chun Liang, Shih-Hong Tsai, Ting-Yun Chang, Yi-Chung Lin, Keh-Yih Su",
      "title": "A Meaning-based English Math Word Problem Solver with Understanding, Reasoning and Explanation.",
      "link": "https://www.aclweb.org/anthology/C16-2032.pdf",
      "year": "2016",
      "venue": "COLING",
      "type": "demo",
      "citation": "7",
      "nlp_task_1": "Question Answering",
      "explainability": "Program Induction, Template-based",
      "visualization": "Natural Language, Raw program",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "annotates with POS tags",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "builds AST",
      "num_preview_img": 1,
      "abstract":"This paper presents a meaning-based statistical math word problem (MWP) solver with understanding, reasoning and explanation. It comprises a web user interface and pipelined modules for analysing the text, transforming both body and question parts into their logic forms, and then performing inference on them. The associated context of each quantity is represented with proposed role-tags (e.g., nsubj, verb, etc.), which provides the flexibility for annotating the extracted math quantity with its associated syntactic and semantic information (which specifies the physical meaning of that quantity). Those role-tags are then used to identify the desired operands and filter out irrelevant quantities (so that the answer can be obtained precisely). Since the physical meaning of each quantity is explicitly represented with those role-tags and used in the inference process, the proposed approach could explain how the answer is obtained in a human comprehensible way."
     },
     {
      "id": "67",
      "authors": "Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal",
      "title": "Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension.",
      "link": "https://www.aclweb.org/anthology/P19-1261.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "4",
      "nlp_task_1": "Question Answering",
      "explainability": "Tree Induction, Provenance",
      "visualization": "Raw Trees, Saliency",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "2",
      "operations": "explainability-aware architecture (??)",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "small (last mile?)",
      "num_preview_img": 2,
      "abstract":"Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage’s output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system’s ability to perform interpretable and accurate reasoning."
     },
     {
      "id": "2",
      "authors": "Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, Gerhard Weikum",
      "title": "QUINT: Interpretable Question Answering over Knowledge Bases.",
      "link": "https://www.aclweb.org/anthology/D17-2011.pdf",
      "year": "2017",
      "venue": "EMNLP",
      "type": "demo",
      "citation": "15",
      "nlp_task_1": "Question Answering",
      "explainability": "Template-based, Provenance, Example-driven",
      "visualization": "Natural Language, other visualization techniques, Raw Examples",
      "main_explainability": "provenance",
      "main_visualization": "natural Language",
      "placement": "2",
      "operations": "template-based, generates query",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "template-based, generates query",
      "abstract": "We present QUINT, a live system for question answering over knowledge bases. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards reformulating the question.",
      "num_preview_img": 2
     },
     {
      "id": "35",
      "authors": "An T. Nguyen, Aditya Kharosekar, Matthew Lease, Byron C. Wallace",
      "title": "Interpretable Joint Graphical Model for Fact-Checking From Crowds.",
      "link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16673/15848",
      "year": "2018",
      "venue": "AAAI",
      "type": "full",
      "citation": "17",
      "nlp_task_1": "",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "Provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "provenance, \"explainability-aware architecture\"",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "entire process",
      "num_preview_img": 1,
      "abstract":"Assessing the veracity of claims made on the Internet is an important, challenging, and timely problem. While automated fact-checking models have potential to help people better assess what they read, we argue such models must be explainable, accurate, and fast to be useful in practice; while prediction accuracy is clearly important, model transparency is critical in order for users to trust the system and integrate their own knowledge with model predictions. To achieve this, we propose a novel probabilistic graphical model (PGM) which combines machine learning with crowd annotations. Nodes in our model correspond to claim veracity, article stance regarding claims, reputation of news sources, and annotator reliabilities. We introduce a fast variational method for parameter estimation. Evaluation across two real-world datasets and three scenarios shows that: (1) joint modeling of sources, claims and crowd annotators in a PGM improves the predictive performance and interpretability for predicting claim veracity; and (2) our variational inference method achieves scalably fast parameter estimation, with only modest degradation in performance compared to Gibbs sampling. Regarding model transparency, we designed and deployed a prototype fact-checker Web tool, including a visual interface for explaining model predictions. Results of a small user study indicate that model explanations improve user satisfaction and trust in model predictions. We share our web demo, model source code, and the 13K crowd labels we collected.1"
     },
     {
      "id": "63",
      "authors": "Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba",
      "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs",
      "link": "https://www.aclweb.org/anthology/P19-1081.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "10",
      "nlp_task_1": "Dialogue and Interactive Systems",
      "explainability": "Provenance",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "provenance",
      "main_visualization": "other",
      "placement": "2",
      "operations": "attention",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": "small",
      "num_preview_img": 1,
      "abstract":"We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning."
     },
     {
      "id": "64",
      "authors": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, Richard Socher",
      "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
      "link": "https://www.aclweb.org/anthology/P19-1487.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "12",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model",
      "visualization": "Natural Language",
      "main_explainability": "Surrogate Model",
      "main_visualization": "Natural Language",
      "placement": "2",
      "operations": "Use pretrained language model  that is finetuned on human-provided explanations",
      "evaluation_metrics": "Human Evaluation + Comparison to ground truth",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning."
     },
     {
      "id": "84",
      "authors": "Danilo Croce, Daniele Rossini, Roberto Basili",
      "title": "Auditing Deep Learning processes through Kernel-based Explanatory Models.",
      "link": "https://www.aclweb.org/anthology/D19-1415.pdf",
      "year": "2019",
      "venue": "EMNLP",
      "type": "full",
      "citation": "0",
      "nlp_task_1": "Syntax: Tagging, Chunking and Parsing",
      "explainability": "Example-driven",
      "visualization": "Raw Examples",
      "main_explainability": "example-driven",
      "main_visualization": "raw examples",
      "placement": "2",
      "operations": "layer-wise relevance propagation, surrogate model",
      "evaluation_metrics": "human evaluation",
      "parts_covered": "saliency via layer-wise relevance propagation",
      "num_preview_img": 1,
      "abstract":"While NLP systems become more pervasive, their accountability gains value as a focal point of effort. Epistemological opaqueness of nonlinear learning methods, such as deep learning models, can be a major drawback for their adoptions. In this paper, we discuss the application of Layerwise Relevance Propagation over a linguistically motivated neural architecture, the Kernel-based Deep Architecture, in order to trace back connections between linguistic properties of input instances and system decisions. Such connections then guide the construction of argumentations on network’s inferences, i.e., explanations based on real examples, semantically related to the input. We propose here a methodology to evaluate the transparency and coherence of analogy-based explanations modeling an audit stage for the system. Quantitative analysis on two semantic tasks, i.e., question classification and semantic role labeling, show that the explanatory capabilities (native in KDAs) are effective and they pave the way to more complex argumentation methods."
     },
     {
      "id": "70",
      "authors": "Alona Sydorova, Nina Porner, Benjamin Roth",
      "title": "Interpretable Question Answering on Knowledge Bases and Text.",
      "link": "https://www.aclweb.org/anthology/P19-1488.pdf",
      "year": "2019",
      "venue": "ACL",
      "type": "full",
      "citation": "1",
      "nlp_task_1": "Question Answering",
      "explainability": "Surrogate Model, Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "surrogate model",
      "main_visualization": "saliency",
      "placement": "2",
      "operations": "attention, input perturbation",
      "evaluation_metrics": "Human Evaluation",
      "parts_covered": "medium",
      "num_preview_img": 2,
      "abstract":"Interpretability of machine learning (ML) models becomes more relevant with their increasing adoption. In this work, we address the interpretability of ML based question answering (QA) models on a combination of knowledge bases (KB) and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of QA. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or attention, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm."
     }
    ],
    "global-post-hoc": [
     {
      "id": "44",
      "authors": "Reid Pryzant, Sugato Basu, Kazoo Sone",
      "title": "Interpretable Neural Architectures for Attributing an Ad's Performance to its Writing Style.",
      "link": "https://www.aclweb.org/anthology/W18-5415/",
      "year": "2018",
      "venue": "EMNLP",
      "type": "workshop",
      "citation": "4",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "class activation mapping + select top k n-grams",
      "evaluation_metrics": "Informal examination",
      "parts_covered": "medium",
      "num_preview_img": 2,
      "abstract":"How much does “free shipping!” help an advertisement’s ability to persuade? This paper presents two methods for performance attribution: finding the degree to which an outcome can be attributed to parts of a text while controlling for potential confounders. Both algorithms are based on interpreting the behaviors and parameters of trained neural networks. One method uses a CNN to encode the text, an adversarial objective function to control for confounders, and projects its weights onto its activations to interpret the importance of each phrase towards each output class. The other method leverages residualization to control for confounds and performs interpretation by aggregating over learned word vectors. We demonstrate these algorithms’ efficacy on 118,000 internet search advertisements and outcomes, finding language indicative of high and low click through rate (CTR) regardless of who the ad is by or what it is for. Our results suggest the proposed algorithms are high performance and data efficient, able to glean actionable insights from fewer than 10,000 data points. We find that quick, easy, and authoritative language is associated with success, while lackluster embellishment is related to failure. These findings agree with the advertising industry’s emperical wisdom, automatically revealing insights which previously required manual A/B testing to discover."
     },
     {
      "id": "43",
      "authors": "Reid Pryzant, Kelly Shen, Dan Jurafsky, Stefan Wager",
      "title": "Deconfounded Lexicon Induction for Interpretable Social Science.",
      "link": "https://nlp.stanford.edu/pubs/pryzant2018lexicon.pdf",
      "year": "2018",
      "venue": "NAACL",
      "type": "full",
      "citation": "6",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Feature Importance",
      "visualization": "Saliency",
      "main_explainability": "feature importance",
      "main_visualization": "saliency",
      "placement": "3",
      "operations": "attention",
      "evaluation_metrics": "Informal Examination",
      "parts_covered": "medium-high",
      "num_preview_img": 2,
      "abstract":"NLP algorithms are increasingly used in computational social science to take linguistic observations and predict outcomes like human preferences or actions. Making these social models transparent and interpretable often requires identifying features in the input that predict outcomes while also controlling for potential confounds. We formalize this need as a new task: inducing a lexicon that is predictive of a set of target variables yet uncorrelated to a set of confounding variables. We introduce two deep learning algorithms for the task. The first uses a bifurcated architecture to separate the explanatory power of the text and confounds. The second uses an adversarial discriminator to force confound-invariant text encodings. Both elicit lexicons from learned weights and attentional scores. We use them to induce lexicons that are predictive of timely responses to consumer complaints (controlling for product), enrollment from course descriptions (controlling for subject), and sales from product descriptions (controlling for seller). In each domain our algorithms pick words that are associated with narrative persuasion; more predictive and less confound-related than those of standard feature weighting and lexicon induction techniques like regression and log odds"
     },
     {
      "id": "30",
      "authors": "Ninghao Liu, Xiao Huang, Jundong Li, Xia Hu",
      "title": "On Interpretation of Network Embedding via Taxonomy Induction",
      "link": "https://dl.acm.org/doi/pdf/10.1145/3219819.3220001",
      "year": "2018",
      "venue": "KDD",
      "type": "full",
      "citation": "8",
      "nlp_task_1": "Computational Social Science and Social Media",
      "explainability": "Surrogate Model",
      "visualization": "Other Visualization Techniques",
      "main_explainability": "surrogate model",
      "main_visualization": "other",
      "placement": "3",
      "operations": "hierarchical clustering on embeddings to infer taxonomy",
      "evaluation_metrics": "Comparison to ground truth",
      "parts_covered": "",
      "num_preview_img": 1,
      "abstract":"Network embedding has been increasingly used in many network analytics applications to generate low-dimensional vector representations, so that many off-the-shelf models can be applied to solve a wide variety of data mining tasks. However, similar to many other machine learning methods, network embedding results remain hard to be understood by users. Each dimension in the embedding space usually does not have any specific meaning, thus it is difficult to comprehend how the embedding instances are distributed in the reconstructed space. In addition, heterogeneous content information may be incorporated into network embedding, so it is challenging to specify which source of information is effective in generating the embedding results. In this paper, we investigate the interpretation of network embedding, aiming to understand how instances are distributed in embedding space, as well as explore the factors that lead to the embedding results. We resort to the posthoc interpretation scheme, so that our approach can be applied to different types of embedding methods. Specifically, the interpretation of network embedding is presented in the form of a taxonomy. Effective objectives and corresponding algorithms are developed towards building the taxonomy. We also design several metrics to evaluate interpretation results. Experiments on real-world datasets from different domains demonstrate that, by comparing with the state-of-the-art alternatives, our approach produces effective and meaningful interpretation to embedding results."
     }
    ],
    "global-self": [
     {
      "id": "42",
      "authors": "Nicolas Prollochs, Stefan Feuerriegel, Dirk Neumann",
      "title": "Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach.",
      "link": "https://www.aclweb.org/anthology/N19-1038.pdf",
      "year": "2019",
      "venue": "NAACL",
      "type": "short",
      "citation": "4",
      "nlp_task_1": "Sentiment Analysis, Stylistic Analysis, and Argument Mining",
      "explainability": "Rule Induction",
      "visualization": "Raw Rules",
      "main_explainability": "induction",
      "main_visualization": "raw declarative",
      "placement": "4",
      "operations": "none",
      "evaluation_metrics": "Human evaluation",
      "parts_covered": "learns DFA rules",
      "num_preview_img": 1,
      "abstract": "Negation scope detection is widely performed as a supervised learning task which relies upon negation labels at word level. This suffers from two key drawbacks: (1) such granular annotations are costly and (2) highly subjective, since, due to the absence of explicit linguistic resolution rules, human annotators often disagree in the perceived negation scopes. To the best of our knowledge, our work presents the first approach that eliminates the need for world-level negation labels, replacing it instead with document-level sentiment annotations. For this, we present a novel strategy for learning fully interpretable negation rules via weak supervision: we apply reinforcement learning to find a policy that reconstructs negation rules from sentiment predictions at document level. Our experiments demonstrate that our approach for weak supervision can effectively learn negation rules. Furthermore, an out-of-sample evaluation via sentiment analysis reveals consistent improvements (of up to 4.66%) over both a sentiment analysis with (i) no negation handling and (ii) the use of word-level annotations from humans. Moreover, the inferred negation rules are fully interpretable."
     }
    ]
   }